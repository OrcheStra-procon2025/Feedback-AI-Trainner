# Procon/src/train.py (æ”¹è‰¯ç‰ˆ)

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from pathlib import Path

# --- ğŸ’¡RTX 40ã‚·ãƒªãƒ¼ã‚ºã®æ€§èƒ½ã‚’æœ€å¤§é™ã«å¼•ãå‡ºã™è¨­å®š ---
# æ··åˆç²¾åº¦å­¦ç¿’ã‚’æœ‰åŠ¹åŒ– (VRAMå‰Šæ¸› & é«˜é€ŸåŒ–)
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# --- ãƒ‘ã‚¹ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š ---
script_path = Path(__file__).resolve()
project_root = script_path.parent.parent
data_path = project_root / 'data' / 'lstm_data_enhanced.npy'
labels_path = project_root / 'data' / 'lstm_labels_enhanced.npy'
model_save_path = project_root / 'data' / 'conducting_model_best.keras' # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å…ˆ

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
EPOCHS = 100 # EarlyStoppingãŒã‚ã‚‹ã®ã§å¤šã‚ã«è¨­å®š
BATCH_SIZE = 64 # VRAMä½¿ç”¨é‡ã«å¿œã˜ã¦èª¿æ•´ (32, 64, 128ãªã©)
VALIDATION_SPLIT = 0.2

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: {data_path}")
    X = np.load(data_path)
    y = np.load(labels_path)
    
    num_samples, timesteps, num_features = X.shape
    num_classes = len(np.unique(y))
    print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†ã€‚å½¢çŠ¶: {X.shape}, ã‚¯ãƒ©ã‚¹æ•°: {num_classes}")

    # 2. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 3. ğŸ’¡åŒæ–¹å‘LSTMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
    model = Sequential([
        Bidirectional(LSTM(128, return_sequences=True), input_shape=(timesteps, num_features)),
        BatchNormalization(),
        Dropout(0.5),
        
        Bidirectional(LSTM(128)),
        BatchNormalization(),
        Dropout(0.5),
        
        Dense(128, activation='relu'),
        Dropout(0.3),
        
        # å‡ºåŠ›å±¤ã®æ´»æ€§åŒ–é–¢æ•°ã¯float32ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€dtypeã‚’æŒ‡å®š
        Dense(num_classes, activation='softmax', dtype='float32')
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.summary()

    # 4. ğŸ’¡åŠ¹ç‡çš„ãªå­¦ç¿’ã®ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
    callbacks = [
        # 10ã‚¨ãƒãƒƒã‚¯æ€§èƒ½ãŒæ”¹å–„ã—ãªã‘ã‚Œã°æ—©æœŸçµ‚äº†
        EarlyStopping(patience=15, monitor='val_loss', restore_best_weights=True),
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®æå¤±ãŒæœ€å°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ä¿å­˜
        ModelCheckpoint(filepath=model_save_path, save_best_only=True, monitor='val_loss', verbose=1),
        # 5ã‚¨ãƒãƒƒã‚¯æ€§èƒ½ãŒæ”¹å–„ã—ãªã‘ã‚Œã°å­¦ç¿’ç‡ã‚’åŠåˆ†ã«
        ReduceLROnPlateau(patience=5, monitor='val_loss', factor=0.5, verbose=1)
    ]

    # 5. ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    print("\nãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_split=VALIDATION_SPLIT,
        callbacks=callbacks
    )

    # 6. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ (EarlyStoppingãŒæœ€è‰¯ã®é‡ã¿ã‚’å¾©å…ƒã—ã¦ãã‚Œã‚‹)
    print("-" * 30)
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æ­£è§£ç‡ (Accuracy): {accuracy:.4f}")
    print(f"ğŸ‰ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ãŒ {model_save_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")

if __name__ == '__main__':
    main()