# Procon/src/create_lstm_dataset.py (æ”¹è‰¯ç‰ˆ)

import numpy as np
import os
from pathlib import Path
from sklearn.preprocessing import StandardScaler
import joblib

# --- å®šæ•°å®šç¾© ---
# å³æ‰‹, å·¦æ‰‹, å³è‚˜, å·¦è‚˜, å³è‚©, å·¦è‚© (ç›¸å¯¾åº§æ¨™ã¨é€Ÿåº¦ã‚’è¨ˆç®—ã™ã‚‹å¯¾è±¡)
KEY_JOINTS_INDICES = [11, 7, 10, 6, 8, 4] 
# åŸºæº–ç‚¹ã¨ãªã‚‹é–¢ç¯€ (ä½“å¹¹ã®ä¸­å¿ƒ)
ROOT_JOINT_INDEX = 1
# 1ã¤ã®å‹•ç”»ã®æœ€å¤§ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
MAX_TIMESTEPS = 150

# --- ãƒ‘ã‚¹è¨­å®š ---
script_path = Path(__file__).resolve()
project_root = script_path.parent.parent
skeleton_folder = project_root / 'data' / 'skeletons'
output_data_path = project_root / 'data' / 'lstm_data_enhanced.npy'
output_labels_path = project_root / 'data' / 'lstm_labels_enhanced.npy'
output_scaler_path = project_root / 'data' / 'lstm_scaler_enhanced.gz'

# (parse_ntu_skeletoné–¢æ•°ã¯å¤‰æ›´ãªã—)
def parse_ntu_skeleton(filepath):
    try:
        with open(filepath, 'r') as f: lines = f.readlines()
        frame_count = int(lines[0])
        all_frames_data, line_idx = [], 1
        for _ in range(frame_count):
            if line_idx >= len(lines): break
            body_count = int(lines[line_idx]); line_idx += 1
            frame_data = np.zeros((25, 3))
            if body_count > 0:
                line_idx += 1
                joint_count = int(lines[line_idx]); line_idx += 1
                for j in range(joint_count):
                    joint_info = lines[line_idx].split()
                    frame_data[j] = [float(coord) for coord in joint_info[0:3]]
                    line_idx += 1
                if body_count > 1: line_idx += (body_count - 1) * (1 + 1 + joint_count)
            all_frames_data.append(frame_data)
        return np.array(all_frames_data)
    except (IOError, ValueError): return None

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def main():
    skeleton_files = [f for f in os.listdir(skeleton_folder) if f.endswith('.skeleton')]
    
    all_sequences = []
    all_labels = []

    for i, filename in enumerate(skeleton_files):
        print(f"å‡¦ç†ä¸­ ({i+1}/{len(skeleton_files)}): {filename}")
        filepath = skeleton_folder / filename
        
        skeleton_data = parse_ntu_skeleton(filepath)
        if skeleton_data is None or len(skeleton_data) == 0: continue

        # 1. é‡è¦ãªé–¢ç¯€ã¨åŸºæº–ç‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        key_joints_data = skeleton_data[:, KEY_JOINTS_INDICES, :]
        root_joint_data = skeleton_data[:, [ROOT_JOINT_INDEX], :] # å½¢çŠ¶ç¶­æŒã®ãŸã‚ãƒªã‚¹ãƒˆã§æŒ‡å®š

        # 2. ç›¸å¯¾åº§æ¨™ã®è¨ˆç®— (ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã‚’åˆ©ç”¨)
        relative_coords = key_joints_data - root_joint_data

        # 3. é€Ÿåº¦ã®è¨ˆç®— (ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®å·®åˆ†)
        # np.diffã¯(N-1)å€‹ã®å·®åˆ†ã‚’è¨ˆç®—ã™ã‚‹ã®ã§ã€å…ˆé ­ã«ã‚¼ãƒ­ã‚’è¿½åŠ ã—ã¦é•·ã•ã‚’åˆã‚ã›ã‚‹
        velocity = np.diff(key_joints_data, axis=0)
        velocity = np.concatenate([np.zeros((1, velocity.shape[1], velocity.shape[2])), velocity], axis=0)

        # 4. ç‰¹å¾´é‡ã‚’çµåˆã—ã€2æ¬¡å…ƒé…åˆ—ã«å¤‰å½¢
        # (ãƒ•ãƒ¬ãƒ¼ãƒ æ•°, é–¢ç¯€æ•°, 3) -> (ãƒ•ãƒ¬ãƒ¼ãƒ æ•°, é–¢ç¯€æ•° * 3)
        relative_coords_flat = relative_coords.reshape(len(relative_coords), -1)
        velocity_flat = velocity.reshape(len(velocity), -1)
        # (ç›¸å¯¾åº§æ¨™ã®ç‰¹å¾´é‡, é€Ÿåº¦ã®ç‰¹å¾´é‡) ã‚’æ¨ªã«çµåˆ
        sequence = np.concatenate([relative_coords_flat, velocity_flat], axis=1)

        # 5. ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§é•·ã•ã‚’æƒãˆã‚‹
        padded_sequence = np.zeros((MAX_TIMESTEPS, sequence.shape[1]))
        length = min(len(sequence), MAX_TIMESTEPS)
        padded_sequence[:length, :] = sequence[:length, :]
        all_sequences.append(padded_sequence)

        # 6. ãƒ©ãƒ™ãƒ«ã®ä½œæˆ
        try:
            label = int(filename.split('A')[1][:3]) - 1
            all_labels.append(label)
        except (IndexError, ValueError):
            del all_sequences[-1]

    X = np.array(all_sequences)
    y = np.array(all_labels)

    # 7. ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    X_reshaped = X.reshape(-1, X.shape[2])
    scaler = StandardScaler()
    X_scaled_reshaped = scaler.fit_transform(X_reshaped)
    X_scaled = X_scaled_reshaped.reshape(X.shape)

    # 8. ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    np.save(output_data_path, X_scaled)
    np.save(output_labels_path, y)
    joblib.dump(scaler, output_scaler_path)

    print("-" * 30)
    print(f"ğŸ‰ é«˜åº¦ãªç‰¹å¾´é‡ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆãŒå®Œäº†ï¼")
    print(f"ç‰¹å¾´é‡æ•°: {X_scaled.shape[2]}")
    print(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ: {output_data_path}")

if __name__ == '__main__':
    main()